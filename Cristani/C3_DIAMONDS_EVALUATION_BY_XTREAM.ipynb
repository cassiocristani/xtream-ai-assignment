{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae6dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "def converting_to_numeral(csv_path, output_path=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Mapping dictionaries for 'cut', 'clarity', and 'color'\n",
    "    cut_mapping = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n",
    "    clarity_mapping = {'I1': 1, 'SI2': 2, 'SI1': 3, 'VS2': 4, 'VS1': 5, 'VVS2': 6, 'VVS1': 7, 'IF': 8}\n",
    "\n",
    "    color_mapping = {'D': 1, 'E': 2, 'F': 3, 'G': 4, 'H': 5, 'I': 6, 'J': 7, 'K': 8, 'L': 9, 'M': 10,\n",
    "                     'N': 11, 'O': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 18, 'V': 19, 'W': 20,\n",
    "                     'X': 21, 'Y': 22, 'Z': 23}\n",
    "\n",
    "    # Convert 'cut', 'clarity', and 'color' to integers\n",
    "    df['cut'] = df['cut'].map(cut_mapping)\n",
    "    df['clarity'] = df['clarity'].map(clarity_mapping)\n",
    "    df['color'] = df['color'].map(color_mapping)\n",
    "\n",
    "    # Calculate 'z_depth' and 'table_width' columns\n",
    "    df['z_depth'] = df['depth'] * df['z'] * 100\n",
    "    df['table_width'] = df['table'] * df['x'] * 100\n",
    "\n",
    "    # Drop rows with NaN values (if any)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Save the preprocessed DataFrame to a new CSV file\n",
    "    if output_path is None:\n",
    "        output_path = csv_path.replace('.csv', '_preprocessed.csv')\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Preprocessed dataset saved at: {output_path}\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def regularize_original_file(dataset_path):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # List of columns to keep\n",
    "    columns_regulirized = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z', 'price']\n",
    "\n",
    "    # Drop columns not in the specified set\n",
    "    data_regularized = data[columns_regularized]\n",
    "\n",
    "    # Save the regularized dataset\n",
    "    regularized_path = dataset_path.replace('.csv', '_regularized.csv')\n",
    "    data_regularized.to_csv(regularized_path, index=False)\n",
    "    return regularized_path\n",
    "\n",
    "\n",
    "def test_reg_file(dataset_path):\n",
    "    columns_regulirized = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z', 'price']\n",
    "    \n",
    "    # Read the original dataset\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    columns_file = list(data.columns)\n",
    "    \n",
    "    if columns_file == columns_regulirized:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def load_and_preprocess_data(dataset_path):\n",
    "    \n",
    "    # Column checker    \n",
    "    data = pd.read_csv(dataset_path)\n",
    "    X = data.drop(columns=['price'])\n",
    "    y = data['price']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test, margin):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    absolute_diff = abs(y_test - y_pred)\n",
    "    within_margin = sum(absolute_diff <= margin * y_test)\n",
    "    percentage_within_margin = (within_margin / len(y_test)) * 100\n",
    "    return mse, percentage_within_margin\n",
    "\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Model saved at {model_path}\")\n",
    "\"\"\"\n",
    "#Divide in 3 tracks of price\n",
    "def detailed_evaluatio_model(model, X_test, y_test, margin):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Long-tailed to deca 1 or deca 10\n",
    "def detailed_evaluatio_model(model, X_test, y_test, margin,tail):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\"\"\"\n",
    "    \n",
    "def plot_model(df_visualization, margin):\n",
    "    underestimate_threshold=0\n",
    "    df_visualization['Underestimated'] = df_visualization['Price Difference'] < underestimate_threshold\n",
    "    df_visualization['Overestimated'] = df_visualization['Price Difference'] > underestimate_threshold\n",
    "\n",
    "    # Filter out samples within a margin\n",
    "    avg_abs_diff_overestimated = df_visualization.loc[df_visualization['Overestimated'], 'Absolute Difference'].mean()\n",
    "    avg_abs_diff_underestimated = df_visualization.loc[df_visualization['Underestimated'], 'Absolute Difference'].mean()\n",
    "    df_filtered_overestimated = df_visualization.loc[df_visualization['Overestimated'] & (df_visualization['Absolute Difference'] > (1 + margin) * avg_abs_diff_overestimated)]\n",
    "    df_filtered_underestimated = df_visualization.loc[df_visualization['Underestimated'] & (df_visualization['Absolute Difference'] > (1 + margin) * avg_abs_diff_underestimated)]\n",
    "\n",
    "    # Count the number of original samples\n",
    "    total_samples = len(df_visualization)\n",
    "\n",
    "    # Count the number of samples after filtering\n",
    "    total_samples_filtered = len(df_filtered_overestimated) + len(df_filtered_underestimated)\n",
    "    total_overestimated_filtered = len(df_filtered_overestimated)\n",
    "    total_underestimated_filtered = len(df_filtered_underestimated)\n",
    "    \n",
    "    # Calculate average and standard deviation for overestimated and underestimated samples after filtering\n",
    "    avg_overestimated_filtered = df_filtered_overestimated['Absolute Difference'].mean()\n",
    "    std_overestimated_filtered = df_filtered_overestimated['Absolute Difference'].std()\n",
    "    avg_underestimated_filtered = df_filtered_underestimated['Absolute Difference'].mean()\n",
    "    std_underestimated_filtered = df_filtered_underestimated['Absolute Difference'].std()\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(f'Total original samples: {total_samples}')\n",
    "    print(f'Total samples after filtering ( {100*margin} %): {total_samples_filtered} ({total_samples_filtered / total_samples * 100:.2f}%)')\n",
    "    print(f'Total overestimated samples after filtering (> {underestimate_threshold} and margin {margin}): {total_overestimated_filtered} '\n",
    "          f'({total_overestimated_filtered / total_samples * 100:.2f}%)')\n",
    "    print(f'Total underestimated samples after filtering (< {underestimate_threshold} and margin {margin}): {total_underestimated_filtered} '\n",
    "          f'({total_underestimated_filtered / total_samples * 100:.2f}%)')\n",
    "    #print(f'Average absolute difference for overestimated samples after filtering: {avg_overestimated_filtered:.2f}')\n",
    "    #print(f'Standard deviation for overestimated samples after filtering: {std_overestimated_filtered:.2f}')\n",
    "    #print(f'Average absolute difference for underestimated samples after filtering: {avg_underestimated_filtered:.2f}')\n",
    "    #print(f'Standard deviation for underestimated samples after filtering: {std_underestimated_filtered:.2f}')\n",
    "\n",
    "    # Plot the differences with actual prices on the x-axis after filtering\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x='Actual Prices', y='Price Difference', hue='Predicted Prices', data=df_filtered_overestimated)\n",
    "    sns.scatterplot(x='Actual Prices', y='Price Difference', hue='Predicted Prices', data=df_filtered_underestimated)\n",
    "\n",
    "    # Set up the plot for the evaluated sample in red\n",
    "    evaluated_sample_index = i  # Assuming 'i' is defined elsewhere\n",
    "    evaluated_sample_actual_price = y_test.iloc[evaluated_sample_index]\n",
    "    evaluated_sample_predicted_price = prediction\n",
    "    evaluated_sample_difference = evaluated_sample_predicted_price - evaluated_sample_actual_price\n",
    "\n",
    "    # Calculate the percentage out from the threshold zero point\n",
    "    percentage_out = (evaluated_sample_difference / avg_overestimated_filtered) * 100\n",
    "\n",
    "    plt.scatter(evaluated_sample_actual_price, evaluated_sample_difference, color='red', marker='X', s=100, label=f'Evalu. Sample Î”{percentage_out:.2f}% ')\n",
    "\n",
    "    # Plot horizontal lines\n",
    "    plt.axhline(y=0, color='black', linestyle='--', label='Zero Difference')\n",
    "    plt.axhline(y=avg_overestimated_filtered, color='red', linestyle='--', label=f'Avg Overestimated: {avg_overestimated_filtered:.2f}')\n",
    "    plt.axhline(y=-avg_underestimated_filtered, color='blue', linestyle='--', label=f'Avg Underestimated: {avg_underestimated_filtered:.2f}')\n",
    "\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Price Difference (Predicted - Actual)')\n",
    "    plt.title(f'Linear Regression: Differences between Predicted and Actual Prices (Margin={margin * 100}%)')\n",
    "    plt.legend(title='Predicted Prices')\n",
    "    plt.show()\n",
    "\n",
    "def explain(model, explainer, X_test, instance_index, feature_names):\n",
    "    instance = X_test.iloc[instance_index].values\n",
    "    prediction = model.predict([instance])[0]\n",
    "\n",
    "    explanation = explainer.explain_instance(instance, model.predict, num_features=len(feature_names))\n",
    "\n",
    "    # Print explanation\n",
    "    print(\"LIME Explanation:\")\n",
    "    print(f\"Predicted Price: {prediction}\")\n",
    "    print(\"Feature Weights:\")\n",
    "    for feature, weight in explanation.as_list():\n",
    "        print(f\"{feature}: {weight}\")\n",
    "    explanation.show_in_notebook()\n",
    "    \n",
    "def train_model_buffer(data_directory, model_path, buffer_sleep_time, margin):\n",
    "    while True:\n",
    "        for dataset_file in os.listdir(data_directory):\n",
    "            dataset_path = os.path.join(data_directory, dataset_file)\n",
    "\n",
    "            # Load and preprocess the data\n",
    "            X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset_path)\n",
    "\n",
    "            # Train the model\n",
    "            model = train_model(X_train, y_train)\n",
    "\n",
    "            # Evaluate the model\n",
    "            mse, percentage_within_margin = evaluate_model(model, X_test, y_test, margin)\n",
    "            print(f'Mean Squared Error: {mse}')\n",
    "            print(f'Percentage of Samples within {margin * 100}%: {percentage_within_margin:.2f}%')\n",
    "\n",
    "            # Save the trained model\n",
    "            save_model(model, model_path)\n",
    "\n",
    "            # Move the processed dataset file to another directory or delete it\n",
    "            processed_dataset_path = os.path.join(data_directory, 'processed', dataset_file)\n",
    "            os.rename(dataset_path, processed_dataset_path)\n",
    "\n",
    "        print(f\"Waiting for {buffer_sleep_time} seconds before checking for new datasets...\")\n",
    "        time.sleep(buffer_sleep_time)\n",
    "    \n",
    "\n",
    "\n",
    "def main():\n",
    "    buffer_time = 3600\n",
    "    margin5 = 0.05\n",
    "    margin10 = 0.1\n",
    "    tail = 0.2\n",
    "    # Specify the directory where new datasets are added\n",
    "    data_directory = \"C:\\\\Users\\\\cassiorodrigo.crisfa\\\\Documents\\\\GitHub\\\\xtream-ai-assignment\\\\Cristani\\\\buffer\"\n",
    "\n",
    "    # Specify the path to save the trained model\n",
    "    model_path = \"C:\\\\Users\\\\cassiorodrigo.crisfa\\\\Documents\\\\GitHub\\\\xtream-ai-assignment\\\\Cristani\\\\model.joblib\"\n",
    "\n",
    "    # Loop through each dataset in the directory\n",
    "    for dataset_file in os.listdir(data_directory):\n",
    "        dataset_path = os.path.join(data_directory, dataset_file)\n",
    "\n",
    "        # Load and preprocess the data\n",
    "        X_train, X_test, y_train, y_test = load_and_preprocess_data(dataset_path)\n",
    "\n",
    "        # Train the model\n",
    "        model = train_model(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mse, percentage = evaluate_model(model, X_test, y_test, margin10)\n",
    "        print(f'Mean Squared Error: {mse}')\n",
    "        print(f'Percentage of Samples within {margin10 * 100}%: {percentage_within_margin:.2f}%')\n",
    "\n",
    "        # Save the trained model\n",
    "        save_model(model, model_path)\n",
    "        \n",
    "        #Explain the model\n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, mode=\"regression\", feature_names=X.columns)\n",
    "        sample = 555\n",
    "        explain(model, explainer, X_test, sample, X.columns)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e6ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
