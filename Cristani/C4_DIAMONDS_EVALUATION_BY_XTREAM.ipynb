{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5972b685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__Diamonds__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cassiorodrigo.crisfa\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from flask import Flask, jsonify, request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import os\n",
    "import time\n",
    "\n",
    "app = Flask(\"__Diamonds__\")\n",
    "teammates = []\n",
    "\n",
    "@app.route('/plot', methods=['GET'])\n",
    "def plot_model(data_path, model, y_test, y_pred, margin, evaluated_sample_index):\n",
    "     # Create a DataFrame for visualization\n",
    "    df_visualization = pd.DataFrame({'Actual Prices': y_test, 'Predicted Prices': y_pred})\n",
    "\n",
    "    # Calculate the differences between predicted and actual prices\n",
    "    df_visualization['Price Difference'] = df_visualization['Predicted Prices'] - df_visualization['Actual Prices']\n",
    "\n",
    "    # Calculate the absolute differences\n",
    "    df_visualization['Absolute Difference'] = abs(df_visualization['Price Difference'])\n",
    "\n",
    "    # Set a threshold for underestimation (adjust as needed)\n",
    "    underestimate_threshold = 0\n",
    "\n",
    "    # Identify underestimated samples\n",
    "    df_visualization['Underestimated'] = df_visualization['Price Difference'] < underestimate_threshold\n",
    "\n",
    "    # Identify overestimated samples\n",
    "    df_visualization['Overestimated'] = df_visualization['Price Difference'] > underestimate_threshold\n",
    "\n",
    "    # Filter out samples within a margin \n",
    "    margin = 0.10 \n",
    "    avg_abs_diff_overestimated = df_visualization.loc[df_visualization['Overestimated'], 'Absolute Difference'].mean()\n",
    "    avg_abs_diff_underestimated = df_visualization.loc[df_visualization['Underestimated'], 'Absolute Difference'].mean()\n",
    "\n",
    "    df_filtered_overestimated = df_visualization.loc[df_visualization['Overestimated'] & (df_visualization['Absolute Difference'] > (1 + margin) * avg_abs_diff_overestimated)]\n",
    "    df_filtered_underestimated = df_visualization.loc[df_visualization['Underestimated'] & (df_visualization['Absolute Difference'] > (1 + margin) * avg_abs_diff_underestimated)]\n",
    "\n",
    "    # Count the number of original samples\n",
    "    total_samples = len(df_visualization)\n",
    "\n",
    "    # Count the number of samples after filtering\n",
    "    total_samples_filtered = len(df_filtered_overestimated) + len(df_filtered_underestimated)\n",
    "    total_overestimated_filtered = len(df_filtered_overestimated)\n",
    "    total_underestimated_filtered = len(df_filtered_underestimated)\n",
    "\n",
    "    # Calculate average and standard deviation for overestimated and underestimated samples after filtering\n",
    "    avg_overestimated_filtered = df_filtered_overestimated['Absolute Difference'].mean()\n",
    "    std_overestimated_filtered = df_filtered_overestimated['Absolute Difference'].std()\n",
    "\n",
    "    avg_underestimated_filtered = df_filtered_underestimated['Absolute Difference'].mean()\n",
    "    std_underestimated_filtered = df_filtered_underestimated['Absolute Difference'].std()\n",
    "\n",
    "    # Print the number of original samples\n",
    "    print(f'Total original samples: {total_samples}')\n",
    "\n",
    "    # Print the number and percentage of samples after filtering\n",
    "    print(f'Total samples after filtering ( {100*margin} %): {total_samples_filtered} ({total_samples_filtered / total_samples * 100:.2f}%)')\n",
    "    print(f'Total overestimated samples after filtering (> {underestimate_threshold} and margin {margin}): {total_overestimated_filtered} '\n",
    "          f'({total_overestimated_filtered / total_samples * 100:.2f}%)')\n",
    "    print(f'Total underestimated samples after filtering (< {underestimate_threshold} and margin {margin}): {total_underestimated_filtered} '\n",
    "          f'({total_underestimated_filtered / total_samples * 100:.2f}%)')\n",
    "\n",
    "    # Print average and standard deviation for overestimated and underestimated samples after filtering\n",
    "    print(f'Average absolute difference for overestimated samples after filtering: {avg_overestimated_filtered:.2f}')\n",
    "    print(f'Standard deviation for overestimated samples after filtering: {std_overestimated_filtered:.2f}')\n",
    "\n",
    "    print(f'Average absolute difference for underestimated samples after filtering: {avg_underestimated_filtered:.2f}')\n",
    "    print(f'Standard deviation for underestimated samples after filtering: {std_underestimated_filtered:.2f}')\n",
    "\n",
    "    # Plot the differences with actual prices on the x-axis after filtering\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.scatterplot(x='Actual Prices', y='Price Difference', hue='Predicted Prices', data=df_filtered_overestimated)\n",
    "    sns.scatterplot(x='Actual Prices', y='Price Difference', hue='Predicted Prices', data=df_filtered_underestimated)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', label='Zero Difference')\n",
    "    plt.axhline(y=avg_overestimated_filtered, color='red', linestyle='--', label=f'Avg Overestimated: {avg_overestimated_filtered:.2f}')\n",
    "    plt.axhline(y=-avg_underestimated_filtered, color='blue', linestyle='--', label=f'Avg Underestimated: {avg_underestimated_filtered:.2f}')\n",
    "\n",
    "    plt.xlabel('Actual Prices')\n",
    "    plt.ylabel('Price Difference (Predicted - Actual)')\n",
    "    plt.title(f'Linear Regression: Differences between Predicted and Actual Prices (Filtered, Margin={margin * 100}%)')\n",
    "    plt.legend(title='Predicted Prices')\n",
    "    plt.show()\n",
    "\n",
    "def converting_to_numeral(csv_path): \n",
    "    #Presupose that there is a backup of the file. OR should create a backup function to do that\n",
    "    \n",
    "    # Read the original CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Mapping dictionaries for 'cut', 'clarity', and 'color'\n",
    "    cut_mapping = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n",
    "    clarity_mapping = {'I1': 1, 'SI2': 2, 'SI1': 3, 'VS2': 4, 'VS1': 5, 'VVS2': 6, 'VVS1': 7, 'IF': 8}\n",
    "    color_mapping = {'D': 1, 'E': 2, 'F': 3, 'G': 4, 'H': 5, 'I': 6, 'J': 7, 'K': 8, 'L': 9, 'M': 10,\n",
    "                     'N': 11, 'O': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 18, 'V': 19, 'W': 20,\n",
    "                     'X': 21, 'Y': 22, 'Z': 23}\n",
    "\n",
    "    # Convert 'cut', 'clarity', and 'color' to integers\n",
    "    df['cut'] = df['cut'].map(cut_mapping)\n",
    "    df['clarity'] = df['clarity'].map(clarity_mapping)\n",
    "    df['color'] = df['color'].map(color_mapping)\n",
    "\n",
    "\n",
    "    # Calculate 'z_depth' and 'table_width' columns\n",
    "    df['z_depth'] = df['depth'] * df['z'] * 100\n",
    "    df['table_width'] = df['table'] * df['x'] * 100\n",
    "\n",
    "    # Drop rows with NaN values (if any)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Create 'color_classification' column \n",
    "    df['color_classification'] = pd.Series(dtype='int')\n",
    "    df.loc[(df['color'] >= 1) & (df['color'] <= 3), 'color_classification'] = 1 # 'Colorless'\n",
    "    df.loc[(df['color'] >= 4) & (df['color'] <= 7), 'color_classification'] = 2 #'Near Colorless'\n",
    "    df.loc[(df['color'] >= 8) & (df['color'] <= 10), 'color_classification'] = 3 #'Faint Yellow'\n",
    "    df.loc[(df['color'] >= 11) & (df['color'] <= 15), 'color_classification'] = 4 #'Very Light Yellow'\n",
    "    df.loc[(df['color'] >= 16) & (df['color'] <= 23), 'color_classification'] = 5 #'Light Yellow'\n",
    "    # Create 'color_classification' column \n",
    "    color_classification_mapping = {'Colorless': 1, 'Near Colorless': 2, 'Faint Yellow': 3, 'Very Light Yellow': 4, 'Light Yellow': 5}\n",
    "    df['color_classification'] = df['color_classification'].map(color_classification_mapping)\n",
    "\n",
    "    return csv_path\n",
    "\n",
    "def regularize_original_file(dataset_path):\n",
    "    # List of columns to keep\n",
    "    columns_regularized = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z', 'price']\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Drop columns not in the specified list\n",
    "    columns_to_drop = [col for col in data.columns if col not in columns_regularized]\n",
    "    data.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    # Save the regularized DataFrame to a new CSV file\n",
    "    data.to_csv(dataset_path, index=False)\n",
    "    \n",
    "    return dataset_path\n",
    "               \n",
    "def test_reg_file(dataset_path):\n",
    "    # columns of regularized file\n",
    "    columns_regularized = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z', 'price']\n",
    "    \n",
    "    data = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Check if columns are equal to the regularized list\n",
    "    if list(data.columns) == columns_regularized:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "@app.route('/explain', methods=['GET'])\n",
    "def explain_model(model, explainer, X_test, instance_index, feature_names):\n",
    "\n",
    "    instance = X_test.iloc[instance_index].values\n",
    "    prediction = model.predict([instance])[0]\n",
    "\n",
    "    explanation = explainer.explain_instance(instance, model.predict, num_features=len(feature_names))\n",
    "\n",
    "    # Print explanation\n",
    "    print(\"LIME Explanation:\")\n",
    "    print(f\"Predicted Price: {prediction}\")\n",
    "    print(\"Feature Weights:\")\n",
    "    for feature, weight in explanation.as_list():\n",
    "        print(f\"{feature}: {weight}\")\n",
    "    explanation.show_in_notebook()\n",
    "    \n",
    "@app.route('/train_buffer', methods=['POST'])\n",
    "def buffer_feeder(data_directory, backup_directory, model_path):\n",
    "    while True:\n",
    "        for dataset_file in os.listdir(data_directory):\n",
    "            dataset_path = os.path.join(data_directory, dataset_file)\n",
    "            \n",
    "            dataset_path = converting_to_numeral(dataset_path)\n",
    "            # Drop columns if not regularized\n",
    "            if test_reg_file(dataset_path) == False:\n",
    "                filepath = regularize_original_file(dataset_path)\n",
    "            \n",
    "            # Load and preprocess the data\n",
    "            X_train, X_test, y_train, y_test = split_price(dataset_path)\n",
    "\n",
    "            # Train the model\n",
    "            model = train_model(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            # Evaluate the model\n",
    "            mse, percentage_within_margin = evaluate_model(model, X_test, y_test, margin)\n",
    "            print(f'Mean Squared Error: {mse}')\n",
    "            print(f'Percentage of Samples within {margin * 100}%: {percentage_within_margin:.2f}%')\n",
    "\n",
    "            os.remove(filepath)\n",
    "            # Save the trained model\n",
    "            save_model(model, model_path)\n",
    "            \n",
    "            \"\"\"#plot_model(dataset_path, model, y_test, y_pred, margin, evaluated_sample_index=1)\n",
    "            #explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, mode=\"regression\", feature_names=X_train.columns)\n",
    "            while True:\n",
    "                user_input = input(\"Enter the number of the samples to explain (or 'x' to exit): \").lower()\n",
    "                if user_input == 'x':\n",
    "                    break  # Break out of the loop if the user wants to exit\n",
    "                try:\n",
    "                    sample = int(user_input)\n",
    "                    # Call the explain_model function\n",
    "                    explain_model(model, explainer, X_test, sample, X_train.columns)\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Please enter a valid integer or 'x' to exit.\")\n",
    "\n",
    "            # Ask if the user needs another explanation for a new sample\n",
    "            another_explanation = input(\"Do you need explain another sample? (y/n): \").lower()\n",
    "            if another_explanation != 'y':\n",
    "                break\n",
    "            \"\"\"\n",
    "            # Move the processed dataset file to another directory or delete it\n",
    "            processed_dataset_path = os.path.join(backup_directory, 'processed', dataset_file)\n",
    "            os.rename(dataset_path, processed_dataset_path)\n",
    "            \n",
    "          \n",
    "\n",
    "\"\"\"     \n",
    "@app.route('/explain', methods=['POST'])\n",
    "def explain_sample():\n",
    "    data = request.get_json()\n",
    "\n",
    "    dataset_path = converting_to_numeral(data['csv_path'])\n",
    "    if not test_reg_file(dataset_path):\n",
    "        filepath = regularize_original_file(dataset_path)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_price(dataset_path)\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, mode=\"regression\", feature_names=X_train.columns)\n",
    "    \n",
    "    sample_index = data['sample_index']\n",
    "    instance = X_test.iloc[sample_index].values\n",
    "\n",
    "    explanation = explainer.explain_instance(instance, model.predict, num_features=len(X_train.columns))\n",
    "    \n",
    "    explanation_data = {\n",
    "        \"predicted_price\": float(model.predict([instance])[0]),\n",
    "        \"explanation\": [{\"feature\": feature, \"weight\": weight} for feature, weight in explanation.as_list()]\n",
    "    }\n",
    "\n",
    "    return jsonify(explanation_data)\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/add_teammate', methods=['POST'])\n",
    "def add_teammate():\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Validate if the required fields are present in the request data\n",
    "    if 'id' not in data or 'name' not in data:\n",
    "        return jsonify({\"error\": \"Missing required fields. Please provide 'id', 'name.\"}), 400\n",
    "\n",
    "    # Create a new teammate\n",
    "    new_teammate = {\n",
    "        'id': data['id'],\n",
    "        'name': data['name'],\n",
    "    }\n",
    "\n",
    "    # Add the new teammate to the data store\n",
    "    teammate.append(new_teammate)\n",
    "\n",
    "    return jsonify({\"message\": \"Customer added successfully\", \"customer\": new_customer}), 201\n",
    "\n",
    "@app.route('/delete_teammate/<int:teammate_id>', methods=['DELETE'])\n",
    "def delete_teammate(customer_id):\n",
    "    global teammate\n",
    "\n",
    "    # Find the customer with the given ID\n",
    "    teammate_to_delete = next((teammate for teammate in teammate if teammate['id'] == teammate_id), None)\n",
    "\n",
    "    if teammate_to_delete:\n",
    "        teammate = [teammate for teammate in teammate if teammate['id'] != teammate_id]\n",
    "        return jsonify({\"message\": f\"Teammate with ID {teammate_id} deleted successfully\"}), 200\n",
    "    else:\n",
    "        return jsonify({\"error\": f\"Teammate with ID {teammate_id} not found\"}), 404\n",
    "\n",
    "\n",
    "@app.route('/teammates/<int:teammate_id>/add_files', methods=['PUT'])\n",
    "def add_file_to_process(teammate_id,file_path, processor_directory):\n",
    "    # Check if the buffer directory exists, create it if not\n",
    "    if not os.path.exists(processor_directory):\n",
    "        os.makedirs(processor_directory)\n",
    "\n",
    "    # Extract the file name from the path\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Create the destination path in the processor directory\n",
    "    destination_path = os.path.join(processor_directory, file_name)\n",
    "\n",
    "    # Move the file to the buffer directory\n",
    "    shutil.move(file_path, destination_path)\n",
    "\n",
    "    print(f\"File '{file_name}' added to the buffer directory.\")\n",
    "    \n",
    "    \n",
    "@app.route('/teammates/<int:teammate_id>/remove_files', methods=['POST'])\n",
    "def remove_files(teammate_id, files, buffer_directory):\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Validate if the required fields are present in the request data\n",
    "    if 'teammate_id' not in data or 'files' not in data:\n",
    "        return jsonify({\"error\": \"Missing required fields. Please provide 'teammate_id' and 'files'.\"}), 400\n",
    "\n",
    "    teammate_id = data['teammate_id']\n",
    "    files_to_remove = data['files']\n",
    "\n",
    "    # Remove files from the buffer directory\n",
    "    remove_files_from_buffer(teammate_id, files_to_remove)\n",
    "\n",
    "    return jsonify({\"message\": \"Files removed successfully\"}), 200\n",
    "\n",
    "@app.route('/teammates/<int:teammate_id>/train', methods=['POST'])\n",
    "def train_buffer(teammate_id, files, buffer_directory):\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c46f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
